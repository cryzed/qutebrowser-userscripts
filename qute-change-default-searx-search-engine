#!/usr/bin/env python

# Dependencies: python-beautifulsoup4, python-requests, python-yaml
# ~/.config/qutebrowser/config.py has to be modified to set the DEFAULT search engine from
# ~/.config/qutebrowser/DEFAULT-search-engine.txt, because currently it's not possible to modify a single key in a
# settings dict, such as: url.searchengines['DEFAULT']

import argparse
import concurrent.futures
import operator
import os
import random
import sys
import threading
import time
import urllib.parse

import bs4
import requests
import yaml

SEARX_URL = 'https://github.com/asciimoo/searx/wiki/Searx-instances'
SEARX_PATH = os.path.join(os.environ['QUTE_CONFIG_DIR'], 'searx.yaml')
DEFAULT_SEARCH_ENGINE_PATH = os.path.join(os.environ['QUTE_CONFIG_DIR'], 'DEFAULT-search-engine.txt')

argument_parser = argparse.ArgumentParser()
argument_parser.add_argument('--selection', '-s', choices=['random', 'fastest'], default='random')
argument_parser.add_argument('--timeout', '-t', type=float, default=3)
argument_parser.add_argument('--threads', '-T', type=int, default=4)

session = requests.session()
executor = None
is_updating = threading.Event()


def get_searx_urls():
    response = session.get(SEARX_URL)
    soup = bs4.BeautifulSoup(response.text, 'html.parser')
    anchors = [li.find('a') for li in soup.find(id='user-content-alive-and-running').find_next('ul')('li')]
    return [urllib.parse.urljoin(anchor['href'], '/') for anchor in anchors]


def rate_searx_instance(url, timeout):
    try:
        start = time.time()
        response = session.get(url, timeout=timeout)
        end = time.time()
        response.raise_for_status()
    except requests.RequestException as exception:
        print(f'[rate_searx_instance] Failed to connect to {url}: {exception}', file=sys.stderr)
        return False

    return end - start


def update_searx_urls(timeout):
    with is_updating:
        print(f'[update_searx_urls] Updating and rating SearX instances...', file=sys.stderr)
        urls = get_searx_urls()
        connected = {}
        for url, delay in zip(urls, executor.map(lambda url: rate_searx_instance(url, timeout), urls)):
            # Connection to SearX instance succeeded
            if delay is not False:
                connected[url] = delay

        with open(SEARX_PATH, 'w', encoding='UTF-8') as file:
            yaml.dump(sorted(connected, key=operator.itemgetter(1)), file)
        print(f'[update_searx_urls] Finished updating and rating {len(connected)} SearX instances', file=sys.stderr)


def main(arguments):
    global executor
    executor = concurrent.futures.ThreadPoolExecutor(max_workers=arguments.threads)

    def load_urls():
        with open(SEARX_PATH, encoding='UTF-8') as file:
            return yaml.load(file)

    if not os.path.exists(SEARX_PATH):
        update_searx_urls(arguments.timeout)
    try:
        urls = load_urls()
    except yaml.YAMLError:
        update_searx_urls(arguments.timeout)
        urls = load_urls()

    url = random.choice(urls) if arguments.selection == 'random' else urls[0]
    query_url = url + '?q={}'
    with open(DEFAULT_SEARCH_ENGINE_PATH, 'w', encoding='UTF-8') as file:
        file.write(query_url)

    # Keep the list of URLs up-to-date
    if not is_updating.is_set():
        threading.Thread(target=update_searx_urls, args=(arguments.timeout,)).start()


arguments = argument_parser.parse_args()
main(arguments)
